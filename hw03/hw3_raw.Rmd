---
title: "MA_Hw_03"
output:
  html_notebook:
    toc: yes
    toc_float: yes
  html_document:
    df_print: paged
    toc: yes
---
  
# Import libraries
The following libraries is what I've used in my code

```{r}
#suppressMessages function to load libraries siliently
suppressMessages(library(plyr))
suppressMessages(library(rethinking))
suppressMessages(library(rstan))
suppressMessages(library(tidyverse))
suppressMessages(library(ggplot2))
suppressMessages(library(gridExtra))
suppressMessages(library(caret))
suppressMessages(library(Metrics))
suppressMessages(library(MLmetrics))
```

# Read the data

Read the data and split them for the following processing

```{r}
train_data = read.csv(file = 'train.csv')

train_x = train_data[, names(train_data) != "Stage1.Output.Measurement0.U.Actual"]
train_y = train_data["Stage1.Output.Measurement0.U.Actual"]

test_data = read.csv(file = 'test.csv')
test_id = test_data[,"ID"]
test_x = test_data [ , (names(test_data )!="ID")]
```

To see the shape of data

```{r}
dim(train_x)
dim(train_y)
```

# EDA & Preprocessing

### "Y" Distridution

Draw the distribution of Y, so I can know how do them distribute

```{r}
train_process = cbind(train_x, train_y)

ggplot()+
  geom_histogram(data = train_process, aes(Stage1.Output.Measurement0.U.Actual), bins = 10)
```

```{r}
ggplot()+
  geom_point(data = train_process, aes(x = 1:nrow(train_process),
                                       y = sort(Stage1.Output.Measurement0.U.Actual)))
```

### Drop Extreme Ys

Throw the extreme y (outliner), so the prediction would not be affected by them

```{r}
train_process = train_process[-which(train_process$Stage1.Output.Measurement0.U.Actual<10),]
train_process = train_process[-which(train_process$Stage1.Output.Measurement0.U.Actual>15),]
```

### Process Time Data

Look at the time_stamp in train and test data

```{r}
train_process["time_stamp"]
test_x["time_stamp"]
```

Transfrom time data into numerical data

```{r}
newtime = c()

for(i in 1:nrow(train_process["time_stamp"])){
  time =  gsub("[/: ]", "" , train_process[i,"time_stamp"], perl=TRUE)
  time = substr(time, nchar(time)-3, nchar(time))
  newtime = c(newtime, time)
} 

newtime = as.numeric(newtime)*100
newtime = as.data.frame(newtime)
colnames(newtime) = "time_stamp"

train_process["time_stamp"] = newtime
train_process["time_stamp"]

newtime = c()

for(i in 1:nrow(test_x["time_stamp"])){
  time =  gsub("[/: ]", "" , test_x[i,"time_stamp"], perl=TRUE)
  time = substr(time, nchar(time)-7, nchar(time)-2)
  newtime = c(newtime, time)
} 

newtime = as.numeric(newtime)
newtime = as.data.frame(newtime) - (newtime[1] - train_process[1, "time_stamp"])
colnames(newtime) = "time_stamp"

test_x["time_stamp"] = newtime
test_x["time_stamp"]
```

```{r}
ggplot()+
  geom_point(data = train_process, aes(x = time_stamp,
                                       y = sort(Stage1.Output.Measurement0.U.Actual)))
```

### Normolization

```{r}
colmeans = colMeans(train_process)
colstds = sapply(train_process, sd, na.rm = TRUE)
```

```{r}
for ( i in 1:(length(colmeans)-1) ){
  train_process[,i] = train_process[,i] - colmeans[i]
  train_process[,i] = train_process[,i] / colstds[i]
}
train_process

for ( i in 1:(length(colmeans)-1) ){
  test_x[,i] = test_x[,i] - colmeans[i]
  test_x[,i] = test_x[,i] / colstds[i]
}
test_x
```

### Collinearity

It is very important to avoid collinearity, because it would cause bad performance.

#### Draw all columns

Draw all the columns, and I find that there might be multicollinearity

```{r}

df_to_draw = train_process[2:(length(train_process)-1)]
colNames = names(df_to_draw)

for(col in colNames){
  plot = df_to_draw %>% 
    ggplot(aes_string(x=col)) +
    geom_histogram(bins = 80)
  
  print(plot)
}
```

#### Tackle collinear column

I find out the collinear columns, and drop them.

```{r}
linecomb = findLinearCombos(train_process)
linecomb
```

```{r}
train_process = train_process[-linecomb$remove]
test_x = test_x[-linecomb$remove]
```

To check again whether there's still collinearity

```{r}
findLinearCombos(train_process)
```


# Model_1 Original

### 0. The stan model
I use matrix to save my train_x, instead of vector which we seen in the tutorial, because there's too many dimension of feature.

```{r}
lr.model = "
data{
    int<lower=0> nrow;
    int<lower=0> ncol;
    matrix[nrow, ncol] train_x; //use matirx rather than vector
    vector[nrow] train_y;
    int<lower=0> nrow_test;
    matrix[nrow_test, ncol] test;
}
parameters{
    real alpha;
    vector[ncol] beta;
    real<lower=0> sigma;
}
model{
    alpha ~ normal(0,10);
    beta ~ normal(0,2); 
    sigma ~ lognormal(0,2);
    
    train_y ~ normal(train_x * beta + alpha, sigma);
    //train_x lies afore beta, because it's a matrix multiplication
}
generated quantities{
    real y_pred[nrow_test];
    vector[nrow_test] mu;
    
    mu = alpha + test * beta;
    y_pred = normal_rng(mu, sigma); //get pred
}
"
```

### 1. Split data into tuning, training and test parts

```{r}
#split all data to three parts, and combine the onehot_fold_all_data and unfold_all_data
processed_x_tuning = train_process[1:991,
                                   (names(train_process)!="Stage1.Output.Measurement0.U.Actual")]
processed_x_train = train_process[992:nrow(train_process),
                                  (names(train_process)!="Stage1.Output.Measurement0.U.Actual")]

tuning_y = train_process[1:991,
                         "Stage1.Output.Measurement0.U.Actual"]
train_y = train_process[992:nrow(train_process),
                        "Stage1.Output.Measurement0.U.Actual"]

tuning_y = as.vector(t(tuning_y)) # The tuning y
train_y = as.vector(t(train_y)) # The training y

processed_x_test = test_x

dim(processed_x_tuning)
dim(processed_x_train)
length(tuning_y)
length(train_y)
dim(processed_x_test)
```

### 2. The Tuning Part
I set tuning part, so that I can know how to adjust my hyper-parameters.  
Also, to be more efficient, I use "optimizing", which is a function in stan, to accelerate the model.

```{r}
lr.data = list(
              train_x = processed_x_train, 
              train_y = train_y, 
              
              nrow = nrow(processed_x_train), 
              ncol = ncol(processed_x_train),
              
              test = processed_x_tuning, 
              nrow_test = nrow(processed_x_tuning))

stan_m = stan_model(model_code = lr.model)
lr.fit = optimizing( # optimizing function to accelerate the model
                stan_m,
                data = lr.data,
                hessian = TRUE)
```

### 3. peek on tuning outcome
I check the prediction of tuning data, and use correlation function to see how well the model does.

```{r}
lr.para = lr.fit$par
tuning_alpha = lr.para[1]
tuning_beta_list = lr.para[2:36]
tuning_pred = lr.para[38:(38+991-1)] ## 991 preds

rmse(actual=tuning_y, predicted=tuning_pred)
```

### 4. Start prediction

```{r}
lr.data = list(
              train_x = processed_x_train, 
              train_y = train_y, 
              
              nrow = nrow(processed_x_train), 
              ncol = ncol(processed_x_train),
              
              test = processed_x_test, 
              nrow_test = nrow(processed_x_test))

stan_m = stan_model(model_code = lr.model)
lr.fit = optimizing(
                stan_m,
                data = lr.data,
                hessian = TRUE)
```

### 5. Give it a look

```{r}
lr.para = lr.fit$par
beta_list = lr.para[ 2 : 36 ]
ans_pred = lr.para[38 : (38+4088-1)] ## 4088 preds

beta_list
ans_pred[1:9]
ans_pred[(length(ans_pred)-4):length(ans_pred)]
length(ans_pred)
```

### 6. Output Part

Make some adjustment to the outcome in order to export to csv 
```{r}
ans_pred = data.frame(ans_pred)
colnames(ans_pred) = c("Stage1.Output.Measurement0.U.Actual")
rownames(ans_pred) = c()

test_id = data.frame(test_id)
colnames(test_id) = c("ID")
rownames(test_id) = c()

sub_data = cbind(test_id, ans_pred)
sub_data
write.csv(sub_data,"test_sub_model_1.csv", row.names = FALSE)
```


# Model_2 PCA

### 1. Do PCA to data

```{r}
all_data_to_pca = rbind(processed_x_tuning, processed_x_train, processed_x_test)
# perform pca
res_pca = prcomp(all_data_to_pca)
pca_features = predict(res_pca, newdata = all_data_to_pca)
# proportion of explained variance 
importance_pca <- summary(res_pca)$importance
barplot(sort(importance_pca[2, importance_pca[2, ] > 0.002], decreasing = FALSE), horiz = TRUE, xlim=c(0,0.14), 
        las=1, cex.names=0.6, main="Explained Variance by Principal Component", xlab="Proportion of explained variance")

colnames(pca_features)
```

### 2. Split data into tuning, training and test parts

```{r}
selectcol_num = 30
pca_x_tuning = pca_features[1:991, 1:selectcol_num]
pca_x_train = pca_features[992:nrow(train_process), 1:selectcol_num]
pca_x_test = pca_features[(nrow(train_process)+1):nrow(pca_features), 1:selectcol_num]

dim(pca_x_tuning)
dim(pca_x_train)
dim(pca_x_test)
```

### 3. The Tuning Part
I set tuning part, so that I can know how to adjust my hyper-parameters.  
Also, to be more efficient, I use "optimizing", which is a function in stan, to accelerate the model.

```{r}
lr.data = list(
              train_x = pca_x_train, 
              train_y = train_y, 
              
              nrow = nrow(pca_x_train), 
              ncol = ncol(pca_x_train),
              
              test = pca_x_tuning, 
              nrow_test = nrow(pca_x_tuning))

stan_m = stan_model(model_code = lr.model)
lr.fit = optimizing( # optimizing function to accelerate the model
                stan_m,
                data = lr.data,
                hessian = TRUE)
```

### 4. peek on tuning outcome
I check the prediction of tuning data, and use correlation function to see how well the model does.

```{r}
lr.para = lr.fit$par
tuning_alpha = lr.para[1]
tuning_beta_list = lr.para[ 2 : (2+selectcol_num-1) ]
tuning_sigma = lr.para[ 2+selectcol_num ]
tuning_pred = lr.para[ (2+selectcol_num+1) : ((2+selectcol_num+1) + 991 - 1) ] ## 991 preds

tuning_pred[1:9]

print(rmse(actual=tuning_y, predicted=tuning_pred))
```

### 5. Start prediction 

```{r}
lr.data = list(
              train_x = pca_x_train, 
              train_y = train_y, 
              
              nrow = nrow(pca_x_train), 
              ncol = ncol(pca_x_train),
              
              test = pca_x_test, 
              nrow_test = nrow(pca_x_test))

stan_m = stan_model(model_code = lr.model)
lr.fit = optimizing(
                stan_m,
                data = lr.data,
                hessian = TRUE)
```

### 6. Give it a look

```{r}
lr.para = lr.fit$par
beta_list = lr.para[ 2 : (2+selectcol_num-1) ]
ans_pred = lr.para[(2+selectcol_num+1) : ((2+selectcol_num+1) + 4088 - 1)] ## 4088 preds

ans_pred[1:9]
length(ans_pred)
```

### 7. Output Part

Make some adjustment to the outcome in order to export to csv 
```{r}
ans_pred = data.frame(ans_pred)
colnames(ans_pred) = c("Stage1.Output.Measurement0.U.Actual")
rownames(ans_pred) = c()

test_id = data.frame(test_id)
colnames(test_id) = c("ID")
rownames(test_id) = c()

sub_data = cbind(test_id, ans_pred)
sub_data
write.csv(sub_data,"test_sub_model_2_PCA.csv", row.names = FALSE)
```


# Model_3 Another Tuning

### 1. Split data in another way

```{r}
#split all data to three parts, and combine the onehot_fold_all_data and unfold_all_data
new_x_tuning1 = train_process[1:400,
                              (names(train_process)!="Stage1.Output.Measurement0.U.Actual")]
new_x_tuning2 = train_process[(nrow(train_process)-400+1):nrow(train_process),
                              (names(train_process)!="Stage1.Output.Measurement0.U.Actual")]

new_x_tuning = rbind(new_x_tuning1, new_x_tuning2)

new_x_train = train_process[401:(nrow(train_process)-400),
                            (names(train_process)!="Stage1.Output.Measurement0.U.Actual")]

new_tuning_y1 = train_process[1:400,
                         "Stage1.Output.Measurement0.U.Actual"]

new_tuning_y2 = train_process[(nrow(train_process)-400+1):nrow(train_process),
                         "Stage1.Output.Measurement0.U.Actual"]

new_tuning_y = rbind(new_tuning_y1, new_tuning_y2)

new_train_y = train_process[401:(nrow(train_process)-400),
                        "Stage1.Output.Measurement0.U.Actual"]

new_tuning_y = as.vector(t(new_tuning_y)) # The tuning y
new_train_y = as.vector(t(new_train_y)) # The training y

new_x_test = test_x

dim(new_x_tuning)
dim(new_x_train)
length(new_tuning_y)
length(new_train_y)
dim(new_x_test)
```

### 2. The Tuning Part
I set tuning part, so that I can know how to adjust my hyper-parameters.  
Also, to be more efficient, I use "optimizing", which is a function in stan, to accelerate the model.

```{r}
lr.data = list(
              train_x = new_x_train, 
              train_y = new_train_y, 
              
              nrow = nrow(new_x_train), 
              ncol = ncol(new_x_train),
              
              test = new_x_tuning, 
              nrow_test = nrow(new_x_tuning))

stan_m = stan_model(model_code = lr.model)
lr.fit = optimizing( # optimizing function to accelerate the model
                stan_m,
                data = lr.data,
                hessian = TRUE)
```

### 3. peek on tuning outcome
I check the prediction of tuning data, and use correlation function to see how well the model does.

```{r}
lr.para = lr.fit$par

new_tuning_beta_list = lr.para[2:36]

new_tuning_pred = lr.para[38:(38+800-1)] ## 800 preds

rmse(actual=new_tuning_y, predicted=new_tuning_pred)
```

### 4. Start prediction 

```{r}
lr.data = list(
              train_x = new_x_train, 
              train_y = new_train_y, 
              
              nrow = nrow(new_x_train), 
              ncol = ncol(new_x_train),
              
              test = processed_x_test, 
              nrow_test = nrow(processed_x_test))

stan_m = stan_model(model_code = lr.model)
lr.fit = optimizing(
                stan_m,
                data = lr.data,
                hessian = TRUE)
```

### 5. Give it a look

```{r}
lr.para = lr.fit$par
beta_list = lr.para[ 2 : 36 ]
ans_pred = lr.para[38 : (38+4088-1)] ## 4088 preds

ans_pred[1:9]
ans_pred[(length(ans_pred)-4):length(ans_pred)]
length(ans_pred)
```

### 6. Output Part

Make some adjustment to the outcome in order to export to csv 
```{r}
ans_pred = data.frame(ans_pred)
colnames(ans_pred) = c("Stage1.Output.Measurement0.U.Actual")
rownames(ans_pred) = c()

test_id = data.frame(test_id)
colnames(test_id) = c("ID")
rownames(test_id) = c()

sub_data = cbind(test_id, ans_pred)
sub_data
write.csv(sub_data,"test_sub_model_3_AnotherTuning.csv", row.names = FALSE)
```


# Model_4 All Data

Because, known by tuning data, the performance in model 2 and 3 are not better, so at the end I concate the tuning and train data in model one in the last model.

### 1. Combine all train data
```{r}
all_x_train = rbind(processed_x_tuning, processed_x_train)

all_train_y = c(tuning_y, train_y)

processed_x_test = test_x

dim(all_x_train)
length(all_train_y)
dim(processed_x_test)
```

### 2. Start prediction 

```{r}
lr.data = list(
              train_x = all_x_train, 
              train_y = all_train_y, 
              
              nrow = nrow(all_x_train), 
              ncol = ncol(all_x_train),
              
              test = processed_x_test, 
              nrow_test = nrow(processed_x_test))

stan_m = stan_model(model_code = lr.model)
lr.fit = optimizing(
                stan_m,
                data = lr.data,
                hessian = TRUE)
```

### 3. Give it a look

```{r}
lr.para = lr.fit$par
beta_list = lr.para[ 2 : 36 ]
ans_pred = lr.para[38 : (38+4088-1)] ## 4088 preds

beta_list
ans_pred[1:9]
ans_pred[(length(ans_pred)-4):length(ans_pred)]
length(ans_pred)
```

### 4. Output Part

Make some adjustment to the outcome in order to export to csv 
```{r}
ans_pred = data.frame(ans_pred)
colnames(ans_pred) = c("Stage1.Output.Measurement0.U.Actual")
rownames(ans_pred) = c()

test_id = data.frame(test_id)
colnames(test_id) = c("ID")
rownames(test_id) = c()

sub_data = cbind(test_id, ans_pred)
sub_data
write.csv(sub_data,"test_sub_model_4_AllData.csv", row.names = FALSE)
```
